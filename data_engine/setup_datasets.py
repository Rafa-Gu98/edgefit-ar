# data_engine/setup_datasets.py
import os
import zipfile
import requests
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
import logging

logger = logging.getLogger(__name__)

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self, data_root: str = "./data_engine/datasets"):
        self.data_root = Path(data_root)
        self.raw_dir = self.data_root / "raw"
        self.processed_dir = self.data_root / "processed"
        
        # åˆ›å»ºç›®å½•
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
    
    def setup_uci_har_dataset(self) -> bool:
        """è®¾ç½®UCI HARæ•°æ®é›†"""
        
        logger.info("è®¾ç½®UCI HARæ•°æ®é›†...")
        
        uci_dir = self.raw_dir / "uci_har"
        uci_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é›†URL
        url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
        zip_path = uci_dir / "UCI_HAR_Dataset.zip"
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
            if not zip_path.exists():
                logger.info("ä¸‹è½½UCI HARæ•°æ®é›†...")
                self._download_file(url, zip_path)
            
            # è§£å‹æ•°æ®é›†
            extract_dir = uci_dir / "UCI HAR Dataset"
            if not extract_dir.exists():
                logger.info("è§£å‹UCI HARæ•°æ®é›†...")
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(uci_dir)
            
            # å¤„ç†æ•°æ®é›†
            self._process_uci_har_dataset(extract_dir)
            
            logger.info("âœ“ UCI HARæ•°æ®é›†è®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"UCI HARæ•°æ®é›†è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def _download_file(self, url: str, destination: Path):
        """ä¸‹è½½æ–‡ä»¶"""
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        with open(destination, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
    
    def _process_uci_har_dataset(self, extract_dir: Path):
        """å¤„ç†UCI HARæ•°æ®é›†"""
        
        # è¯»å–è®­ç»ƒæ•°æ®
        train_x = np.loadtxt(extract_dir / "train" / "X_train.txt")
        train_y = np.loadtxt(extract_dir / "train" / "y_train.txt")
        
        # è¯»å–æµ‹è¯•æ•°æ®
        test_x = np.loadtxt(extract_dir / "test" / "X_test.txt")
        test_y = np.loadtxt(extract_dir / "test" / "y_test.txt")
        
        # åˆå¹¶æ•°æ®
        all_x = np.vstack([train_x, test_x])
        all_y = np.hstack([train_y, test_y])
        
        # æ´»åŠ¨æ ‡ç­¾æ˜ å°„
        activity_labels = {
            1: 'WALKING',
            2: 'WALKING_UPSTAIRS', 
            3: 'WALKING_DOWNSTAIRS',
            4: 'SITTING',
            5: 'STANDING',
            6: 'LAYING'
        }
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processed_dir = self.processed_dir / "uci_har"
        processed_dir.mkdir(exist_ok=True)
        
        np.save(processed_dir / "features.npy", all_x)
        np.save(processed_dir / "labels.npy", all_y)
        
        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        import json
        with open(processed_dir / "label_mapping.json", 'w') as f:
            json.dump(activity_labels, f, indent=2)
        
        logger.info(f"å¤„ç†å®Œæˆ: {all_x.shape[0]} ä¸ªæ ·æœ¬, {all_x.shape[1]} ä¸ªç‰¹å¾")
    
    def create_synthetic_fitness_dataset(self) -> bool:
        """åˆ›å»ºåˆæˆå¥èº«æ•°æ®é›†"""
        
        logger.info("åˆ›å»ºåˆæˆå¥èº«æ•°æ®é›†...")
        
        try:
            from hardware_simulator.data_generator import SyntheticDataGenerator
            
            generator = SyntheticDataGenerator(str(self.raw_dir / "synthetic"))
            
            # ç”Ÿæˆå„ç§è¿åŠ¨çš„æ•°æ®é›†
            exercises = ["squat", "pushup", "plank", "lunge", "jumping_jack"]
            
            all_data = []
            all_labels = []
            
            for i, exercise in enumerate(exercises):
                # ç”Ÿæˆæ•°æ®é›†
                dataset_path = generator.generate_exercise_dataset(
                    exercise_type=exercise,
                    num_subjects=30,
                    reps_per_subject=15
                )
                
                # è¯»å–ç”Ÿæˆçš„æ•°æ®
                with open(dataset_path, 'r') as f:
                    import json
                    exercise_data = json.load(f)
                
                # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
                for sample in exercise_data:
                    features = (
                        sample['accelerometer'] + 
                        sample['gyroscope'] + 
                        sample['magnetometer']
                    )
                    all_data.append(features)
                    all_labels.append(i)  # æ•°å­—æ ‡ç­¾
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            features_array = np.array(all_data)
            labels_array = np.array(all_labels)
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            synthetic_dir = self.processed_dir / "synthetic_fitness"
            synthetic_dir.mkdir(exist_ok=True)
            
            np.save(synthetic_dir / "features.npy", features_array)
            np.save(synthetic_dir / "labels.npy", labels_array)
            
            # ä¿å­˜æ ‡ç­¾æ˜ å°„
            label_mapping = {i: exercise for i, exercise in enumerate(exercises)}
            with open(synthetic_dir / "label_mapping.json", 'w') as f:
                json.dump(label_mapping, f, indent=2)
            
            logger.info(f"âœ“ åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ: {features_array.shape[0]} ä¸ªæ ·æœ¬")
            return True
            
        except Exception as e:
            logger.error(f"åˆæˆæ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    def get_dataset_info(self) -> Dict[str, Dict]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        
        info = {}
        
        for dataset_dir in self.processed_dir.iterdir():
            if dataset_dir.is_dir():
                features_path = dataset_dir / "features.npy"
                labels_path = dataset_dir / "labels.npy"
                mapping_path = dataset_dir / "label_mapping.json"
                
                if all(p.exists() for p in [features_path, labels_path]):
                    features = np.load(features_path)
                    labels = np.load(labels_path)
                    
                    dataset_info = {
                        "samples": features.shape[0],
                        "features": features.shape[1],
                        "classes": len(np.unique(labels)),
                        "path": str(dataset_dir)
                    }
                    
                    if mapping_path.exists():
                        with open(mapping_path, 'r') as f:
                            import json
                            dataset_info["label_mapping"] = json.load(f)
                    
                    info[dataset_dir.name] = dataset_info
        
        return info

def main():
    """ä¸»å‡½æ•° - è®¾ç½®æ‰€æœ‰æ•°æ®é›†"""
    
    logging.basicConfig(level=logging.INFO)
    
    manager = DatasetManager()
    
    # è®¾ç½®UCI HARæ•°æ®é›†
    manager.setup_uci_har_dataset()
    
    # åˆ›å»ºåˆæˆå¥èº«æ•°æ®é›†
    manager.create_synthetic_fitness_dataset()
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    info = manager.get_dataset_info()
    
    print("\n" + "="*50)
    print("æ•°æ®é›†è®¾ç½®å®Œæˆ")
    print("="*50)
    
    for name, details in info.items():
        print(f"\nğŸ“Š {name.upper()}")
        print(f"   æ ·æœ¬æ•°: {details['samples']}")
        print(f"   ç‰¹å¾æ•°: {details['features']}")
        print(f"   ç±»åˆ«æ•°: {details['classes']}")
        if 'label_mapping' in details:
            print(f"   æ ‡ç­¾: {list(details['label_mapping'].values())}")

if __name__ == "__main__":
    main()